{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72832b4f-34d0-4d13-9e9a-51a964490f04",
   "metadata": {},
   "source": [
    "# FinalProject_Part2_DataMining-py\n",
    "**oriya shapira**\n",
    "- link to the project page in GitHub - https://github.com/OriyaShapira/FinalProject_DataMining.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f17d6d7-a365-455d-80ad-225a5e169851",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oriya\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "#!pip install fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d83454b-5b76-4508-ab84-e2325fee7bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def missing_val_from_description(df, column, description, keyword_lst):\n",
    "    # Fill missing values in a specified column based on keywords in the 'Description' column.\n",
    "    condition = df[column].isna() | ~df[column].isin(keyword_lst)\n",
    "    for keyword in keyword_lst:\n",
    "        df.loc[condition & df[description].str.contains(keyword, na=False), column] = keyword\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dff2af9-9bf3-4db1-853b-a6d809bfc492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def match_closest_val(value, choices, threshold=80):\n",
    "    # This function find the closest matching string from a list of choices based on a given value.\n",
    "    best_match, best_score = process.extractOne(value, choices, scorer=fuzz.partial_ratio)\n",
    "    if best_score >= threshold:\n",
    "        return best_match\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91163a59-5071-4216-8512-4dd918b16cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_supply_info_for_model_year(supply_info, manufactor, model, year):\n",
    "    # This function returns the supply score for a given car model and year, using fuzzy matching to find the best match. \n",
    "    # Since the car model provided by the writer in a freeform manner, the '=' was replaced with 'similar to'to avoid spelling mistakes\n",
    "    matching_supply = [supply for supply in supply_info if \n",
    "                       (supply['manufactor'] == manufactor) and\n",
    "                       (fuzz.partial_ratio(model.lower(), supply['model'].lower()) >= 70) and\n",
    "                       (supply['Year'] == int(year))]\n",
    "    \n",
    "    if matching_supply:\n",
    "        return matching_supply[0]['Supply_score']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b34a08-bb08-4a97-b64f-c3e99ef8d988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_supply_info(keyword):\n",
    "    # This function returns a list of dictionaries containing the car model, year, and supply score from the gov API page.\n",
    "    supply_info = []\n",
    "    try:\n",
    "        url = 'https://data.gov.il/api/3/action/datastore_search?resource_id=5e87a7a1-2f6f-41c1-8aec-7216d52a6cf6'\n",
    "        response = requests.get(url)\n",
    "        if not response.status_code == 200:\n",
    "            print (f\"Could not fetch data from API website. Got status code: {response.status_code}\")\n",
    "        results_page = response.json() \n",
    "        for item in results_page['result']['records']:\n",
    "            if item['tozar'] in keyword:\n",
    "                supply_info.append({'manufactor': item['tozar'],\n",
    "                                    'Year': item['shnat_yitzur'],\n",
    "                                   'model': item['kinuy_mishari'],\n",
    "                                   'Supply_score': item['mispar_rechavim_pailim']})\n",
    "        return supply_info\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get Supply info for {keyword}. Got error: {str(e)}\")\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04697752-1a84-41b2-9ab6-3277a7ced781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_Cre_date_in_days(cre_date):\n",
    "    # This function takes a date and returns the number of days that have passed since that date.\n",
    "    try:\n",
    "        if not cre_date == None:\n",
    "            cre_date = pd.to_datetime(cre_date, dayfirst=True)\n",
    "            delta = (datetime.datetime.now() - cre_date).days\n",
    "            return int(delta)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a0c808-4708-4c84-896d-2e639971e943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sale_score(description, good_keywords_lst, bad_keywords_lst):\n",
    "    # This function return a score based on words mentiond in the description\n",
    "    try:\n",
    "        score = 0\n",
    "        for Gword in good_keywords_lst:\n",
    "            if Gword in description:\n",
    "                score += 1\n",
    "        for Bword in bad_keywords_lst:\n",
    "            if Bword in description:\n",
    "                score -= 1\n",
    "        return score\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7761fb64-b361-46c4-9d3d-2474aa4d6da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    # Prepare and clean the data for feature engineering and modeling.\n",
    "    # Correcting data types and preparing them for feature engineering\n",
    "    categoricals = ['Prev_ownership', 'Curr_ownership', 'Engine_type', 'Gear']\n",
    "    strings = ['Area','City','Color','Description','manufactor','model']\n",
    "    integers = ['Year','Hand', 'capacity_Engine', 'Pic_num', 'Km', 'Test', 'Supply_score']\n",
    "    \n",
    "    df[categoricals] = df[categoricals].astype('category')\n",
    "    df[strings] = df[strings].astype(pd.StringDtype())\n",
    "    df[integers] = df[integers].replace({',': ''}, regex=True).apply(pd.to_numeric, errors='coerce').astype('Int64')\n",
    "    \n",
    "    # Fill missing values in 'Curr_ownership' based on keywords in the 'Description' column\n",
    "    ownership_lst = ['פרטית', 'ליסינג', 'חברה', 'השכרה']\n",
    "    df = missing_val_from_description(df, 'Curr_ownership', 'Description', ownership_lst)\n",
    "    df['Curr_ownership'].replace({'פרטית':'private', 'ליסינג':'leasing', 'חברה':'company', 'השכרה':'renting','אחר':'other', 'לא מוגדר':'other'},inplace = True)\n",
    "    df['Curr_ownership'] = df['Curr_ownership'].fillna('other') #fill none values as 'other'\n",
    "    \n",
    "    # Fill missing values in 'Engine_type' based on keywords in the 'Description' column\n",
    "    Engine_type_lst = ['דיזל','גז', 'בנזין', 'חשמלי', 'היברידי']\n",
    "    df = missing_val_from_description(df, 'Engine_type', 'Description', Engine_type_lst)\n",
    "    df['Engine_type'].replace({'היברידי':'hybrid','היבריד':'hybrid','בנזין':'gasoline','דיזל':'diesel','גז':'gas','טורבו דיזל':'turbo diesel','חשמלי':'electric'}, inplace = True)\n",
    "    \n",
    "    # Add new categories to 'Gear' and fill missing values based on keywords in the 'Description' column\n",
    "    new_categories = ['ידני', 'הילוכים'] # Adding common words describing gear type to the list of categories. \n",
    "    df['Gear'] = df['Gear'].cat.add_categories(new_categories)\n",
    "    Gear_type_lst = ['הילוכים', 'אוטומט','רובוטית','אוטומטית','טיפטרוניק','ידני','ידנית']\n",
    "    df = missing_val_from_description(df, 'Gear', 'Description', Gear_type_lst)\n",
    "    df['Gear'].replace({'הילוכים':'Manual','ידני':'Manual', 'אוטומט':'Automatic','רובוטית':'Automatic','אוטומטית':'Automatic','טיפטרוניק':'Tiptronic','ידנית':'Manual','לא מוגדר':None}, inplace = True)\n",
    "    df['Gear'] = df['Gear'].fillna(df['Gear'].mode()[0]) # Empty vals wes filled using the most frequent val\n",
    "    \n",
    "    # Normalize city and area names and correct values based on frequency\n",
    "    df['City'] = df['City'].str.replace('[,\\\\.]', '', regex=True)\n",
    "    df['City'] = df['City'].str.replace('יי', 'י', regex=True)\n",
    "    \n",
    "    frequent_areas = df['Area'].value_counts()\n",
    "    correct_areas = frequent_areas[frequent_areas > 5].index.tolist()\n",
    "    df['Area'] = df['Area'].apply(lambda x: match_closest_val(x, correct_areas) if pd.notna(x) else x)\n",
    "    \n",
    "    # Create a dictionary of cities by area to fill missing 'Area' values based on 'City'\n",
    "    city_dict = {area: list(group['City'].dropna().unique()) for area, group in df.groupby('Area')}\n",
    "    df['Area'] = df.apply(lambda row: next((area for area, cities in city_dict.items() if row['City'] in cities), row['Area']), axis=1)\n",
    "\n",
    "    # Fill missing 'Km' and 'capacity_Engine' values based on group by method\n",
    "    df['Km'] = df.groupby(['Year'])['Km'].transform(lambda x: x.fillna(round(x.mean())))\n",
    "    df['capacity_Engine']=df.groupby(['manufactor','model', 'Year'])['capacity_Engine'].fillna(df['capacity_Engine'].mode()[0])\n",
    "\n",
    "    # Correct frequent values in 'model' column\n",
    "    df['model'] = df['model'].str.strip()\n",
    "    df['model'] = df.apply(lambda row: re.sub(r'\\(.*?\\)', '', row['model']).strip(), axis=1)\n",
    "    df['model'] = df.apply(lambda row: row['model'].replace(row['manufactor'], '').strip(), axis=1)\n",
    "    df.loc[(df['manufactor'] == 'מיני') & (df['model'] == 'קופר'), 'manufactor'] = 'ב מ וו'\n",
    "    df.loc[(df['manufactor'] == 'מיני') & (df['model'] == 'קופר'), 'model'] = 'MINI COOPER'\n",
    "    df.loc[(df['manufactor'] == 'מיני') & (df['model'] != 'קופר'), 'manufactor'] = 'ב מ וו'\n",
    "    df.loc[(df['manufactor'] == 'מיני') & (df['model'] != 'קופר'), 'model'] = 'MINI ' + df['model']\n",
    "    df['manufactor'].replace({'Lexsus':'לקסוס','מאזדה':'מזדה'},inplace = True)\n",
    "    df['model'].replace({'אוקטביה':'octavia','פיקנטו':'picanto','גולף':'golf','קורולה':'corolla','ספארק':'spark'}, inplace = True)\n",
    "    \n",
    "    # Add supply score for missing rows based on model name similarity\n",
    "    suplly_df = df.loc[pd.isna(df['Supply_score'])]\n",
    "    model_dict = {manufactor: list(group['model'].dropna().unique()) for manufactor, group in suplly_df.groupby('manufactor')}\n",
    "    supply_info= get_supply_info(model_dict.keys())\n",
    "    df['Supply_score'] = df.apply(lambda row: get_supply_info_for_model_year(supply_info, row['manufactor'], row['model'], row['Year'])\n",
    "                               if pd.isna(row['Supply_score']) else row['Supply_score'], axis=1)\n",
    "    df['Supply_score'] = df.groupby(['manufactor', 'Year'])['Supply_score'].transform(lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    # Add a column for the count of each 'Area'\n",
    "    df['Cre_days'] = df['Cre_date'].apply(get_Cre_date_in_days)\n",
    "    df['Cre_days'] = df['Cre_days'].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    # Add a new score column based on keywords in the 'Description'\n",
    "    good_keywords_lst = ['ללא','מתוחזק','מצב מצויין','מצב טוב', 'חדש', 'ללא תקלות', 'שמור','אמין']\n",
    "    bad_keywords_lst = ['תקול','תיקון','רועד','רעידות','שריטה','מכה','לא תקין','ישן','לא עובד','מקולקל','שרוט','שבר','מעט']\n",
    "    df['own_Score'] = df['Description'].apply(lambda x: sale_score(x, good_keywords_lst, bad_keywords_lst))\n",
    "    \n",
    "    # Combine 'Gear' and 'Engine_type' into a new column\n",
    "    df['Gear_Engine'] = df.apply(lambda row: str(row['Gear']) + ':' + str(row['Engine_type']), axis=1)\n",
    "    df['Area_Count'] = df.groupby('Area')['Area'].transform('count')\n",
    "    \n",
    "    # Drop unnecessary columns and rows with missing values\n",
    "    df = df.drop(columns=['Prev_ownership','Repub_date','Color','Test','Pic_num','Cre_date','Description'])\n",
    "    df = df.dropna(subset=['Supply_score','Area','Engine_type'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5909ecf1-26f9-40ae-81a3-a7ba8e4c7e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import data from GitHub\n",
    "url_df = 'https://raw.githubusercontent.com/OriyaShapira/FinalProject_DataMining/main/dataset.csv'\n",
    "df = pd.read_csv(url_df)\n",
    "df = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03dcd0c7-ff40-4046-bee2-595508f61535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE: 16552.769416671574\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "X = df.drop(columns='Price')\n",
    "y = df['Price']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "numeric_features = ['Year', 'Hand', 'capacity_Engine', 'Km', 'Supply_score', 'Cre_days', 'Area_Count', 'own_Score']\n",
    "categorical_features = X.select_dtypes(include=['category', 'object', 'string', 'bool']).columns\n",
    "\n",
    "# Ensure test set has the same columns as the training set\n",
    "X_test = X_test[numeric_features + list(categorical_features)]\n",
    "\n",
    "# Preprocessing pipeline for numerical features\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "\n",
    "# Preprocessing pipeline for categorical features\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])  # Convert categorical values to one-hot encoded variables\n",
    "\n",
    "# Combined preprocessing pipeline\n",
    "preprocessor = make_column_transformer(\n",
    "    (numerical_pipeline, numeric_features),\n",
    "    (categorical_pipeline, categorical_features),\n",
    "    remainder='drop')  # Drop any other columns that are not specified\n",
    "\n",
    "# Elastic Net pipeline\n",
    "elastic_net_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Apply the preprocessing pipeline\n",
    "    ('elastic_net', ElasticNet(random_state=42))])  # Apply the Elastic Net regression model\n",
    "\n",
    "# Cross-validation setup\n",
    "cross_val = KFold(n_splits=10)  # 10-fold cross-validation\n",
    "\n",
    "# Perform cross-validation to evaluate the model\n",
    "scores = cross_val_score(elastic_net_pipeline, X_train, y_train, cv=cross_val, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-scores)  # Calculate RMSE from the negative mean squared error scores\n",
    "print(\"Average RMSE:\", np.mean(rmse_scores))  # Print the average RMSE\n",
    "\n",
    "# Fit the model on the entire training set\n",
    "elastic_net_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extract the fitted Elastic Net model\n",
    "elastic_net_model = elastic_net_pipeline.named_steps['elastic_net']\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = elastic_net_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5ca04-67a3-44ff-866a-5333f78469b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
